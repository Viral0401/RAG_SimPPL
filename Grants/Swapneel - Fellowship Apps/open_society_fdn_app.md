It is the year 2030, the year of quantum language models. Scientific advancements have made superconducting, trillion-transistor chips available as commodity hardware so we run personalized AI models on our household devices--in our own languages and directed by our own volition. All information we receive is not only through platform-based algorithmic curation but is also filtered by our pocket AI, who serves as our personal arbiter of truth and factuality. In this information ecosystem, is it feasible to expect any standards to be applied for AI governance agreed upon by everyone? 
What is truth and what normative standards should be applied to quantify it when there is no centralized authority with the ability to share facts? We revisit the role of journalistic integrity in this universe: in particular, we will discuss what a post-truth world driven largely by algorithmic decision-making would look like in a democratized universe of personalized AI companions. Such companions are ubiquitous, and useful, but also potentially biased, influenceable, and possible to adversarially game. What value does media integrity and public opinion hold when a large swath of it relies on algorithmically created and curated content? What about when AI makes mistakes? Do we hold AI accountable, it's creators, or is it just collateral damage, part of the negative externalities?